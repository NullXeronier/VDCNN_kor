{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data_in/train.csv\").iloc[:, 1:].dropna(how = 'any')\n",
    "train_data.index = range(len(train_data))\n",
    "test_data = pd.read_csv(\"./data_in/test.csv\").iloc[:, 1:]\n",
    "submission = pd.read_csv(\"./data_in/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39992, 2), (5000, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['data'] = train_data['data'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category    0\n",
      "data        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_data['data'].replace('', np.nan, inplace=True)\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [category, data]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[train_data.data.isnull()][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39992\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna(how = 'any')\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 테스트용 샘플의 개수 : 5000\n"
     ]
    }
   ],
   "source": [
    "# test_data.drop_duplicates(subset = ['data'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['data'] = test_data['data'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 한글 이외 삭제\n",
    "# test_data['data'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "# test_data = test_data.dropna(how='any') # Null 값 제거\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(test_data['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecab Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다','을']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take nouns by Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39992/39992 [00:57<00:00, 700.36it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "\n",
    "text = list(train_data['data'])\n",
    "\n",
    "for i in tqdm(range(len(text))):\n",
    "  temp_X = []\n",
    "  temp_X = mecab.nouns(text[i]) # 토큰화\n",
    "  temp_X = [word for word in temp_X if not word in stop_words] # 불용어 제거\n",
    "  temp_X = [word for word in temp_X if len(word) > 1]\n",
    "  X_train.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:07<00:00, 705.42it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "\n",
    "text = list(test_data['data'])\n",
    "\n",
    "for i in tqdm(range(len(text))):\n",
    "  temp_X = []\n",
    "  temp_X = mecab.nouns(text[i]) # 토큰화\n",
    "  temp_X = [word for word in temp_X if not word in stop_words] # 불용어 제거\n",
    "  # temp_X = [word for word in temp_X if len(word) > 1]\n",
    "  X_test.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,  ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 45833\n",
      "등장 빈도가 10번 이하인 희귀 단어의 수: 33115\n",
      "단어 집합에서 희귀 단어의 비율: 72.25143455588768\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.134287559411337\n"
     ]
    }
   ],
   "source": [
    "threshold = 11\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 12719\n"
     ]
    }
   ],
   "source": [
    "vocab_size = total_cnt - rare_cnt + 1 # 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거. 0번 패딩 토큰을 고려하여 +1\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 2048) # num_words = vocab_size\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "token_X_train = tokenizer.texts_to_sequences(X_train)\n",
    "token_X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(np.array(train_data['category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(token_X_train) if len(sentence) < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39926\n",
      "39926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/TF2_37/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# 빈 샘플들을 제거\n",
    "del_X_train = np.delete(token_X_train, drop_train, axis=0)\n",
    "del_y_train = np.delete(y_train, drop_train, axis=0)\n",
    "print(len(del_X_train))\n",
    "print(len(del_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data의 최대 길이 : 4859\n",
      "train data의 평균 길이 : 57.76852176526574\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('train data의 최대 길이 :',max(len(l) for l in del_X_train))\n",
    "print('train data의 평균 길이 :',sum(map(len, del_X_train))/len(del_X_train))\n",
    "plt.hist([len(s) for s in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data의 최대 길이 :  3155\n",
      "test data의 평균 길이 :  57.2548\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"test data의 최대 길이 : \", max(len(l) for l in token_X_test))\n",
    "print(\"test data의 평균 길이 : \", sum(map(len, token_X_test))/ len(token_X_test))\n",
    "plt.hist([len(s) for s in token_X_test], bins=50)\n",
    "plt.xlabel('length of Data')\n",
    "plt.ylabel('number of Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if(len(s) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 2048 이하인 샘플의 비율: 99.99499073285578\n"
     ]
    }
   ],
   "source": [
    "max_len = 2048\n",
    "below_threshold_len(max_len, del_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_X_train = pad_sequences(del_X_train, maxlen = max_len)\n",
    "pad_X_test = pad_sequences(token_X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from k_maxpooling import *\n",
    "from vdcnn import *\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_x_train, del_y_train // pad_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_X_train[pad_X_train>2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "Model: \"VDCNN\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 2048, 256)    524288      inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "temp_conv (Conv1D)              (None, 2048, 64)     49216       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 2048, 64)     12352       temp_conv[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 2048, 64)     256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 2048, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 2048, 64)     12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 2048, 64)     256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 2048, 64)     0           batch_normalization_17[0][0]     \n",
      "                                                                 temp_conv[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 2048, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 2048, 64)     12352       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 2048, 64)     256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 2048, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 2048, 64)     12352       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 2048, 64)     256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "shortcut_conv1d_1 (Conv1D)      (None, 1024, 64)     4160        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pool_1 (KMaxPooling)            (None, 1024, 64)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "shortcut_batch_normalization_1  (None, 1024, 64)     256         shortcut_conv1d_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 1024, 64)     0           pool_1[0][0]                     \n",
      "                                                                 shortcut_batch_normalization_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 1024, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "1_1_conv_1 (Conv1D)             (None, 1024, 128)    8320        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1_1_batch_normalization_1 (Batc (None, 1024, 128)    512         1_1_conv_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1024, 128)    49280       1_1_batch_normalization_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 1024, 128)    512         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1024, 128)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 1024, 128)    49280       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 1024, 128)    512         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 1024, 128)    0           batch_normalization_21[0][0]     \n",
      "                                                                 1_1_batch_normalization_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 1024, 128)    0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 1024, 128)    49280       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 1024, 128)    512         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 1024, 128)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 1024, 128)    49280       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 1024, 128)    512         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "shortcut_conv1d_2 (Conv1D)      (None, 512, 128)     16512       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pool_2 (KMaxPooling)            (None, 512, 128)     0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "shortcut_batch_normalization_2  (None, 512, 128)     512         shortcut_conv1d_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 512, 128)     0           pool_2[0][0]                     \n",
      "                                                                 shortcut_batch_normalization_2[0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 512, 128)     0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "1_1_conv_2 (Conv1D)             (None, 512, 256)     33024       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1_1_batch_normalization_2 (Batc (None, 512, 256)     1024        1_1_conv_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 512, 256)     196864      1_1_batch_normalization_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 512, 256)     1024        conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 512, 256)     0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 512, 256)     196864      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 512, 256)     1024        conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 512, 256)     0           batch_normalization_25[0][0]     \n",
      "                                                                 1_1_batch_normalization_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 512, 256)     0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 512, 256)     196864      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 512, 256)     1024        conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 512, 256)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 512, 256)     196864      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 512, 256)     1024        conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "shortcut_conv1d_3 (Conv1D)      (None, 256, 256)     65792       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pool_3 (KMaxPooling)            (None, 256, 256)     0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "shortcut_batch_normalization_3  (None, 256, 256)     1024        shortcut_conv1d_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 256, 256)     0           pool_3[0][0]                     \n",
      "                                                                 shortcut_batch_normalization_3[0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 256, 256)     0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "1_1_conv_3 (Conv1D)             (None, 256, 512)     131584      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1_1_batch_normalization_3 (Batc (None, 256, 512)     2048        1_1_conv_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 256, 512)     786944      1_1_batch_normalization_3[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 256, 512)     2048        conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 256, 512)     0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 256, 512)     786944      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 256, 512)     2048        conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 256, 512)     0           batch_normalization_29[0][0]     \n",
      "                                                                 1_1_batch_normalization_3[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 256, 512)     0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 256, 512)     786944      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 256, 512)     2048        conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 256, 512)     0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 256, 512)     786944      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 256, 512)     2048        conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 256, 512)     0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_1 (KMaxPooling)   (None, 8, 512)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           k_max_pooling_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2048)         8390656     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2048)         4196352     dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 3)            6147        dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 17,628,547\n",
      "Trainable params: 17,618,179\n",
      "Non-trainable params: 10,368\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.1033 - accuracy: 0.7333\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.80240, saving model to ./checkpoints/vdcnn_weights.ckpt\n",
      "500/500 [==============================] - 113s 225ms/step - loss: 1.1033 - accuracy: 0.7333 - val_loss: 0.5448 - val_accuracy: 0.8024\n",
      "Epoch 2/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.3957 - accuracy: 0.8526\n",
      "Epoch 00002: val_accuracy did not improve from 0.80240\n",
      "500/500 [==============================] - 111s 221ms/step - loss: 0.3959 - accuracy: 0.8526 - val_loss: 1.8515 - val_accuracy: 0.4694\n",
      "Epoch 3/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.3751 - accuracy: 0.8611\n",
      "Epoch 00003: val_accuracy improved from 0.80240 to 0.84210, saving model to ./checkpoints/vdcnn_weights.ckpt\n",
      "500/500 [==============================] - 111s 223ms/step - loss: 0.3754 - accuracy: 0.8611 - val_loss: 0.4390 - val_accuracy: 0.8421\n",
      "Epoch 4/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.3400 - accuracy: 0.8706\n",
      "Epoch 00004: val_accuracy did not improve from 0.84210\n",
      "500/500 [==============================] - 111s 221ms/step - loss: 0.3401 - accuracy: 0.8706 - val_loss: 0.5292 - val_accuracy: 0.8022\n",
      "Epoch 5/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.8830\n",
      "Epoch 00005: val_accuracy improved from 0.84210 to 0.85199, saving model to ./checkpoints/vdcnn_weights.ckpt\n",
      "500/500 [==============================] - 111s 223ms/step - loss: 0.2965 - accuracy: 0.8830 - val_loss: 0.3974 - val_accuracy: 0.8520\n",
      "Epoch 6/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.2680 - accuracy: 0.8967\n",
      "Epoch 00006: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 222ms/step - loss: 0.2680 - accuracy: 0.8967 - val_loss: 0.4358 - val_accuracy: 0.8428\n",
      "Epoch 7/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.2380 - accuracy: 0.9060\n",
      "Epoch 00007: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 222ms/step - loss: 0.2382 - accuracy: 0.9060 - val_loss: 0.4342 - val_accuracy: 0.8357\n",
      "Epoch 8/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.2187 - accuracy: 0.9144\n",
      "Epoch 00008: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 221ms/step - loss: 0.2189 - accuracy: 0.9143 - val_loss: 0.5060 - val_accuracy: 0.8347\n",
      "Epoch 9/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1954 - accuracy: 0.9238\n",
      "Epoch 00009: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 221ms/step - loss: 0.1956 - accuracy: 0.9238 - val_loss: 1.6058 - val_accuracy: 0.5401\n",
      "Epoch 10/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1790 - accuracy: 0.9292\n",
      "Epoch 00010: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 110s 221ms/step - loss: 0.1794 - accuracy: 0.9291 - val_loss: 0.9551 - val_accuracy: 0.7963\n",
      "Epoch 11/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1839 - accuracy: 0.9291\n",
      "Epoch 00011: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 221ms/step - loss: 0.1839 - accuracy: 0.9291 - val_loss: 0.5948 - val_accuracy: 0.8360\n",
      "Epoch 12/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1389 - accuracy: 0.9460\n",
      "Epoch 00012: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 221ms/step - loss: 0.1396 - accuracy: 0.9459 - val_loss: 1.1794 - val_accuracy: 0.7643\n",
      "Epoch 13/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.2340 - accuracy: 0.9120\n",
      "Epoch 00013: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 110s 221ms/step - loss: 0.2340 - accuracy: 0.9120 - val_loss: 4.6358 - val_accuracy: 0.3759\n",
      "Epoch 14/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1675 - accuracy: 0.9359\n",
      "Epoch 00014: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 222ms/step - loss: 0.1683 - accuracy: 0.9358 - val_loss: 0.8434 - val_accuracy: 0.7120\n",
      "Epoch 15/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1353 - accuracy: 0.9481\n",
      "Epoch 00015: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 221ms/step - loss: 0.1359 - accuracy: 0.9480 - val_loss: 0.7702 - val_accuracy: 0.7893\n",
      "Epoch 16/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9475\n",
      "Epoch 00016: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 222ms/step - loss: 0.1396 - accuracy: 0.9475 - val_loss: 2.0684 - val_accuracy: 0.5465\n",
      "Epoch 17/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1449 - accuracy: 0.9445\n",
      "Epoch 00017: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 222ms/step - loss: 0.1449 - accuracy: 0.9445 - val_loss: 1.1351 - val_accuracy: 0.7403\n",
      "Epoch 18/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1014 - accuracy: 0.9597\n",
      "Epoch 00018: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 222ms/step - loss: 0.1014 - accuracy: 0.9597 - val_loss: 1.0404 - val_accuracy: 0.8380\n",
      "Epoch 19/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9714\n",
      "Epoch 00019: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 110s 220ms/step - loss: 0.0718 - accuracy: 0.9714 - val_loss: 1.0846 - val_accuracy: 0.7308\n",
      "Epoch 20/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.0915 - accuracy: 0.9633\n",
      "Epoch 00020: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 222ms/step - loss: 0.0916 - accuracy: 0.9633 - val_loss: 0.6568 - val_accuracy: 0.8281\n",
      "Epoch 21/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9713\n",
      "Epoch 00021: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 222ms/step - loss: 0.0711 - accuracy: 0.9713 - val_loss: 0.9089 - val_accuracy: 0.8233\n",
      "Epoch 22/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.0579 - accuracy: 0.9763\n",
      "Epoch 00022: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 223ms/step - loss: 0.0616 - accuracy: 0.9762 - val_loss: 2.5025 - val_accuracy: 0.7915\n",
      "Epoch 23/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1509 - accuracy: 0.9429\n",
      "Epoch 00023: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 222ms/step - loss: 0.1509 - accuracy: 0.9429 - val_loss: 0.5780 - val_accuracy: 0.8431\n",
      "Epoch 24/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.0656 - accuracy: 0.9737\n",
      "Epoch 00024: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 222ms/step - loss: 0.0657 - accuracy: 0.9737 - val_loss: 1.3565 - val_accuracy: 0.7670\n",
      "Epoch 25/25\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.0695 - accuracy: 0.9730\n",
      "Epoch 00025: val_accuracy did not improve from 0.85199\n",
      "500/500 [==============================] - 111s 222ms/step - loss: 0.0700 - accuracy: 0.9729 - val_loss: 1.3886 - val_accuracy: 0.7616\n",
      "------------------------------\n",
      "2020-12-06 16_13_14: Done training.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def train(x_train, y_train):\n",
    "    \n",
    "    # Init Keras Model here\n",
    "    model = VDCNN(num_classes=3, \n",
    "                  sequence_length=2048,\n",
    "                  shortcut=True,\n",
    "                  pool_type='k_max', \n",
    "                  sorted=False, \n",
    "                  use_bias=False, embedding_dim=256, depth=17)\n",
    "    \n",
    "    # SGD(lr=0.006, momentum=0.99), adam\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    # Trainer\n",
    "    # Tensorboard and extra callback to support steps history\n",
    "    # tensorboard = TensorBoard(log_dir='./logs', histogram_freq=50, write_graph=True, write_images=True)\n",
    "    checkpoint_path = \"./checkpoints/vdcnn_weights.ckpt\"\n",
    "    checkpointer = ModelCheckpoint(filepath=checkpoint_path, #period=1,\n",
    "                                   verbose=10, save_best_only=True, save_weights_only=True, \n",
    "                                   mode='max', monitor='val_accuracy')\n",
    "    \n",
    "    # loss_history = custom_callbacks.loss_history(model, tensorboard)\n",
    "    # evaluate_step = custom_callbacks.evaluate_step(model, checkpointer, 100, BATCH_SIZE, x_test, y_test)\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(x_train, y_train, batch_size=64, epochs=25, validation_split=0.2, \n",
    "              verbose=1, callbacks=[checkpointer])\n",
    "    print('-'*30)\n",
    "    time_str = datetime.now().strftime('%Y-%m-%d %H_%M_%S')\n",
    "    print(f'{time_str}: Done training.')\n",
    "    K.clear_session()\n",
    "    \n",
    "    model.load_weights(checkpoint_path)\n",
    "    return model\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    model = train(x_train=pad_X_train, y_train=del_y_train)\n",
    "    model.save('vdcnn_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  category\n",
       "0      0         0\n",
       "1      1         2\n",
       "2      2         1\n",
       "3      3         0\n",
       "4      4         2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = load_model('vdcnn_try.h5')\n",
    "y_pred = model.predict(pad_X_test)\n",
    "sample_submission = pd.read_csv('./data_in/sample_submission.csv')\n",
    "sample_submission['category'] = np.argmax(y_pred, axis=-1)\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('submission_'+ datetime.now().strftime('%Y-%m-%d %H_%M_%S') + '.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tf2_37)",
   "language": "python",
   "name": "conda_tf2_37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
